{"cells":[{"cell_type":"code","source":["# ==============================================================================\n","# STANDARD LIBRARIES & SPARK CONFIGURATION\n","# ==============================================================================\n","\n","# mssparkutils: Microsoft Fabric utility toolset for file system operations,\n","# secret retrieval, and cross-notebook orchestration.\n","from notebookutils import mssparkutils\n","\n","# col: Function to select and manipulate specific DataFrame columns.\n","# trim: Removes leading and trailing whitespace from strings.\n","# to_timestamp: Converts a string column to a Timestamp type based on a format.\n","from pyspark.sql.functions import col, trim, to_timestamp\n","\n","# F: Standard alias for PySpark SQL functions, providing a centralized\n","# namespace for transformations (aggregations, math, logic).\n","from pyspark.sql import functions as F"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:41.4434206Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:41.4446018Z","execution_finish_time":"2026-02-01T02:58:41.7647361Z","parent_msg_id":"a498dc6c-34ba-44a5-b8d8-81b181b373f8"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cec0f50e-246a-4805-95d3-9a7943762c2a"},{"cell_type":"code","source":["# Function to interface with the Spark catalog and retrieve data\n","def extract_table(table_name: str):\n","    \"\"\"\n","    Reads a table from the Spark catalog and loads it into a DataFrame.\n","\n","    Args:\n","        table_name (str): The name of the table to be extracted (e.g., 'gold_sales_data').\n","\n","    Returns:\n","        pyspark.sql.dataframe.DataFrame: A Spark DataFrame containing the table data.\n","    \"\"\"\n","    df = spark.read.table(table_name)\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:41.5867783Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:41.7671291Z","execution_finish_time":"2026-02-01T02:58:42.0882299Z","parent_msg_id":"b0863e70-0357-4366-b429-ed71fb3ab020"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6dc86ea-26f4-4591-bffc-8935f4f83b0e"},{"cell_type":"code","source":["# Function to automate whitespace removal across the entire DataFrame\n","def trim_all_string_cols(df):\n","    \"\"\"\n","    Identifies all string-type columns and applies the SQL trim function to remove \n","    leading and trailing whitespace.\n","    \n","    Args:\n","        df (pyspark.sql.DataFrame): The input DataFrame to be cleaned.\n","        \n","    Returns:\n","        pyspark.sql.DataFrame: A new DataFrame with trimmed string values.\n","    \"\"\"\n","\n","    # Get all columns that are of type 'string'\n","    string_columns = [c for c, t in df.dtypes if t == 'string']\n","\n","    # Apply trim to those columns specifically\n","    df_trimmed = df.withColumns({c: trim(col(c)) for c in string_columns})\n","\n","    # Return the transformed DataFrame with cleaned string data\n","    return df_trimmed"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:42.030269Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:42.0903172Z","execution_finish_time":"2026-02-01T02:58:42.3910992Z","parent_msg_id":"4e38bc75-784a-4222-8da7-fcdfa605482a"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d910dd6-af83-4693-95be-68b0986260c6"},{"cell_type":"code","source":["# Wrapper function to orchestrate data cleaning tasks\n","def trim_data(df):\n","\n","    trimmed_df = trim_all_string_cols(df)\n","    # group_df = trimmed_df.groupBy(['geolocation_zip_code_prefix', 'geolocation_state']) \\\n","    #     .agg({'geolocation_lat': 'mean', 'geolocation_lng':'mean'})\n","\n","    return trimmed_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:42.5805709Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:42.5816864Z","execution_finish_time":"2026-02-01T02:58:42.8912304Z","parent_msg_id":"18da8b88-c4b1-4f54-bedd-13ebeb73a96e"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 20, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c76b6d4-8d9d-432c-8a3e-87dcf202e484"},{"cell_type":"code","source":["# Function to persist DataFrame results back to the Lakehouse as a Delta Table\n","def load_df_to_delta(df, table_name: str, mode: str = \"overwrite\"):\n","    \"\"\"\n","    Load and saves dataframe data as a Delta Table.\n","    \n","    Args:\n","        df (spark DataFrame): df to be loaded to delta table.\n","        table_name: name of destination table \n","        mode (str): 'overwrite' to replace the table, default.\n","    \"\"\"\n","    try:\n","        # 1. Save as a Delta Table \n","        df.write.format(\"delta\") \\\n","            .mode(mode) \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","\n","        # Confirm successful completion\n","        print(f\"Table '{table_name}' loaded successfully !\")\n","        \n","    except Exception as e:\n","        # Error handling to catch issues like schema mismatches or permission errors\n","        print(f\"Error processing table {table_name}: {str(e)}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:43.3092483Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:43.3104681Z","execution_finish_time":"2026-02-01T02:58:43.6067496Z","parent_msg_id":"618911c0-8119-40d4-8b0f-103e30b743fd"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 21, Finished, Available, Finished)"},"metadata":{}}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ff4a38cd-06db-4323-bc09-8c97e2895e7a"},{"cell_type":"code","source":["# Function to perform Data Quality (DQ) checks for null values in key seller fields\n","def check_null_columns(df):\n","    \"\"\"\n","    Scans specific seller identity and location columns for NULL values,\n","    counts the occurrences, and previews the problematic records.\n","    \n","    Args:\n","        df (pyspark.sql.DataFrame): The DataFrame to be inspected.\n","    \"\"\"\n","\n","    # Identify rows where any of the mandatory seller fields are null.\n","    null_df = df.filter(\n","            F.col(\"seller_id\").isNull() | \n","            F.col(\"seller_zip_code_prefix\").isNull() | \n","            F.col(\"seller_city\").isNull() |             \n","            F.col(\"seller_state\").isNull()\n","        )\n","\n","    null_count = null_df.count()\n","    print(f\"Records with null columns: {null_count}\")\n","\n","    # If the filter captured any records, display them for manual inspectio\n","    if null_count > 0:\n","        # Reuses the filtered logic\n","        null_df.show()\n","pass"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:43.9171488Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:43.9183206Z","execution_finish_time":"2026-02-01T02:58:44.2042023Z","parent_msg_id":"73d874f9-e1d7-4e40-ba3c-d785376d5489"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":20,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8204c3a8-0cc1-430c-8942-397237e55e6f"},{"cell_type":"code","source":["# Function acting as a central hub for data cleaning and filtering logic\n","def clean_dirty_data(df):\n","    #Clean by removing dirty data\n","\n","    print(\"Cleaning script executed ! \")\n","\n","    return df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:44.5892704Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:44.5904516Z","execution_finish_time":"2026-02-01T02:58:44.9761526Z","parent_msg_id":"ee506984-ca1c-4367-a510-7a9ce4a83c9b"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 23, Finished, Available, Finished)"},"metadata":{}}],"execution_count":21,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32bbec80-f1cc-46ec-bf1a-97d6a08fdd72"},{"cell_type":"code","source":["# Function to validate foreign key relationships between the source and a reference table\n","def check_referencing_key(df, ref_table_name: str, df_col: str, ref_col: str, error_col: str): \n","    \"\"\"\n","    Identifies records in the source DataFrame whose keys are missing from a \n","    reference table. Rows with missing keys are flagged with 'Y' in a new column.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The source DataFrame to validate.\n","        ref_table_name (str): The name of the catalog table used for validation.\n","        df_col (str): The key column name in the source DataFrame.\n","        ref_col (str): The key column name in the reference table.\n","        error_col (str): The name of the flag column to be created (e.g., 'invalid_zip_flag').\n","\n","    Returns:\n","        pyspark.sql.DataFrame: The DataFrame with the added error flag column.\n","    \"\"\"\n","\n","    # 1. Load the reference table from the Spark Catalog\n","    ref_df = spark.read.table(ref_table_name)    \n","\n","    # 2. Perform a Left Join to find mismatches\n","    # Aliasing as \"main\" and \"ref\" prevents ambiguous column errors during selection.\n","    invalid_keys_df = df.alias(\"main\").join(\n","        ref_df.alias(\"ref\"), \n","        df[df_col] == ref_df[ref_col], \n","        how=\"left\"\n","    )\n","\n","    # 3. Identify records where the join result is NULL (meaning no match was found)\n","    invalid_count = invalid_keys_df.filter(col(\"ref.\" + ref_col).isNull()).count()\n","\n","    if invalid_count > 0:\n","        print(f\"Found {invalid_count} Keys: {df_col} not present in the referencing table: {ref_table_name}\")\n","        print(f\"{error_col} column set to Y\")\n","\n","        # Display the specific values from the main table that failed the lookup\n","        invalid_keys_df.select(col(\"main.\" + df_col)).filter(col(\"ref.\" + ref_col).isNull()).show()\n","\n","        # 4. Apply the 'Y' flag to invalid rows and remove auxiliary reference columns\n","        # F.when() assigns 'Y' to null matches; .select(\"main.*\") keeps only original source data.\n","        df = invalid_keys_df.withColumn(\n","        error_col,\n","        F.when(F.col(\"ref.\" + ref_col).isNull(), \"Y\")\n","        ).select(\"main.*\", error_col)\n","\n","    else:\n","        # Confirms all data has a valid corresponding entry in the reference table\n","        print(f\"All {df_col} found to be valid and present in the referencing table: {ref_table_name}.\\n\")\n","\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":24,"statement_ids":[24],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:45.1695867Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:45.1708306Z","execution_finish_time":"2026-02-01T02:58:45.4620125Z","parent_msg_id":"e0661816-fa79-48f1-846a-0c366d25db1d"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 24, Finished, Available, Finished)"},"metadata":{}}],"execution_count":22,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cf859afe-03b5-4e43-ad65-79e7b6be5947"},{"cell_type":"code","source":["# Function to verify data uniqueness and identify duplicate primary keys\n","def check_records_count(df, col_name: str): \n","    \"\"\"\n","    Validates the integrity of a specific column by comparing total row counts \n","    against distinct row counts and identifying any duplicate values.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The DataFrame to be audited.\n","        col_name (str): The name of the column that should be unique (e.g., 'customer_id').\n","    \"\"\"\n","    \n","    # Check that after cleaning records is now unique and dirty data (if any) removed\n","    print(f\"Total count: {df.select(col_name).count()}\")\n","    print(f\"Distinct count: {df.select(col_name).distinct().count()}\")\n","\n","    # can add assert statement\n","    \n","    print(\"Records to investigate (if any not unique key)\")\n","    df.groupBy(col_name).count().filter(\"count > 1\").show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":25,"statement_ids":[25],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:45.7794828Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:45.7807495Z","execution_finish_time":"2026-02-01T02:58:46.0974199Z","parent_msg_id":"9d519a11-0317-4530-8cd2-94a262847583"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 25, Finished, Available, Finished)"},"metadata":{}}],"execution_count":23,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ac5d5bb-0fe1-4fad-9cdf-98f31f1f32bc"},{"cell_type":"code","source":["# ==============================================================================\n","# MAIN ETL PIPELINE EXECUTION: Sellers Bronze to Silver\n","# ==============================================================================\n","\n","# 1. Configuration: Define source and destination entry points in the Lakehouse\n","from_table_name = 'BronzeLakeHouse.dbo.sellers_bronze'\n","to_table_name = 'sellers_silver'\n","\n","# 2. Extract: Load the raw seller data from the Bronze layer\n","df = extract_table(from_table_name)\n","\n","# 3. Transform (Cleaning): Remove whitespace from all string columns (ID, City, State, etc.)\n","df = trim_data(df)\n","\n","# 4. Data Quality Check: Print count of rows with null values in mandatory seller fields\n","check_null_columns(df)\n","\n","# 5. Referential Integrity Check A: Validate seller zip codes against geolocation master data\n","# Adds 'not_in_geo_flag' column set to 'Y' for missing references.\n","df = check_referencing_key(df, 'geolocation_silver', 'seller_zip_code_prefix', 'geolocation_zip_code_prefix', 'not_in_geo_flag')\n","\n","# 6. Referential Integrity Check B: Validate seller existence within the order items records\n","# Adds 'not_in_order_items_flag' column set to 'Y' for sellers without associated items.\n","df = check_referencing_key(df, 'BronzeLakeHouse.dbo.order_items_bronze', 'seller_id', 'seller_id', 'not_in_order_items_flag')\n","\n","# 7. Uniqueness Validation: Ensure 'seller_id' remains a unique primary key\n","check_records_count(df, 'seller_id')\n","\n","# 8. Conditional Cleaning: Optional hook to filter or drop records based on flags\n","# >> If dirty Record to Clean here. \n","# df = clean_dirty_data(df)\n","\n","# 9. Load: Persist the final validated DataFrame as a Delta table in the Silver layer\n","load_df_to_delta(df, to_table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":26,"statement_ids":[26],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:46.4056506Z","session_start_time":null,"execution_start_time":"2026-02-01T02:58:46.4068277Z","execution_finish_time":"2026-02-01T03:00:02.8762811Z","parent_msg_id":"d194c895-26b7-4377-baa7-a028b71787ab"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 26, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Records with null columns: 0\nFound 7 Keys: seller_zip_code_prefix not present in the referencing table: geolocation_silver\nnot_in_geo_flag column set to Y\n+----------------------+\n|seller_zip_code_prefix|\n+----------------------+\n|                 82040|\n|                 91901|\n|                 72580|\n|                 02285|\n|                 07412|\n|                 71551|\n|                 37708|\n+----------------------+\n\nAll seller_id found to be valid and present in the referencing table: BronzeLakeHouse.dbo.order_items_bronze.\n\nTotal count: 3095\nDistinct count: 3095\nRecords to investigate (if any not unique key)\n+---------+-----+\n|seller_id|count|\n+---------+-----+\n+---------+-----+\n\nTable 'sellers_silver' loaded successfully !\n"]}],"execution_count":24,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4b50c8ea-fca9-4106-b778-9cd78c4cb0a7"},{"cell_type":"code","source":["# spark.sql(\"REFRESH TABLE order_items_silver\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:46.9816535Z","session_start_time":null,"execution_start_time":"2026-02-01T03:00:02.8789109Z","execution_finish_time":"2026-02-01T03:00:03.2294538Z","parent_msg_id":"caf1aecd-1209-461f-8e13-09779935049a"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 27, Finished, Available, Finished)"},"metadata":{}}],"execution_count":25,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e222bc18-2731-40e5-adbf-289e30040649"},{"cell_type":"code","source":["# df.createOrReplaceTempView(\"test\")\n","# check_df = spark.sql(\"\"\" select count(*) from test where not_in_geo_flag = 'Y' \"\"\")\n","# # check_df = spark.sql(\"\"\" select count(*) from test where not_in_orders_flag IS NULL \"\"\")\n","\n","# check_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[28],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:47.5093265Z","session_start_time":null,"execution_start_time":"2026-02-01T03:00:03.231642Z","execution_finish_time":"2026-02-01T03:00:03.5697783Z","parent_msg_id":"8a4c0a05-0fd5-4622-9cb5-cad057989a20"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 28, Finished, Available, Finished)"},"metadata":{}}],"execution_count":26,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a2be470-96f8-4267-bebd-3393e4f0de90"},{"cell_type":"code","source":["# ==============================================================================\n","# SESSION TERMINATION\n","# ==============================================================================\n","\n","# mssparkutils.session.stop(): Manually shuts down the active Spark session.\n","# This releases the allocated compute (V-Cores) back to the capacity pool.\n","\n","mssparkutils.session.stop()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":29,"statement_ids":[29],"state":"finished","livy_statement_state":"available","session_id":"349e4202-63e3-4f78-b954-ab3739ad03ee","normalized_state":"finished","queued_time":"2026-02-01T02:58:47.8140163Z","session_start_time":null,"execution_start_time":"2026-02-01T03:00:03.5719586Z","execution_finish_time":"2026-02-01T03:00:05.0637828Z","parent_msg_id":"414a76b1-c2a7-4566-94b0-35c3bfa5816f"},"text/plain":"StatementMeta(, 349e4202-63e3-4f78-b954-ab3739ad03ee, 29, Finished, Available, Finished)"},"metadata":{}}],"execution_count":27,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a98609d6-d52e-4001-b1b2-df5eb74236d1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"791a60f4-f8f8-4ea5-9519-0426798180f4"},{"id":"dc1b06fe-c8d2-48ee-9a55-d0149ad78dcd"}],"default_lakehouse":"dc1b06fe-c8d2-48ee-9a55-d0149ad78dcd","default_lakehouse_name":"SilverLakeHouse","default_lakehouse_workspace_id":"7d303a82-1fe7-4f4b-82d6-d23015b8c472"}}},"nbformat":4,"nbformat_minor":5}