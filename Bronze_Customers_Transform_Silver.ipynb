{"cells":[{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","\n","#Standard Libraries\n","from notebookutils import mssparkutils\n","from pyspark.sql.functions import col, trim\n","from pyspark.sql import functions as F"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:00.2260455Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:00.2272338Z","execution_finish_time":"2026-02-01T02:20:00.535345Z","parent_msg_id":"490bc085-2389-4ed5-9afd-c1868a123308"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cec0f50e-246a-4805-95d3-9a7943762c2a"},{"cell_type":"code","source":["# Function to interface with the Spark catalog and retrieve data\n","def extract_table(table_name: str):\n","    \"\"\"\n","    Reads a table from the Spark catalog and loads it into a DataFrame.\n","\n","    Args:\n","        table_name (str): The name of the table to be extracted (e.g., 'gold_sales_data').\n","\n","    Returns:\n","        pyspark.sql.dataframe.DataFrame: A Spark DataFrame containing the table data.\n","    \"\"\"    \n","    df = spark.read.table(table_name)\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:00.3942872Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:00.5374287Z","execution_finish_time":"2026-02-01T02:20:00.8310665Z","parent_msg_id":"cf9f77ad-fefc-44f9-8f4d-c57a31259dbe"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6dc86ea-26f4-4591-bffc-8935f4f83b0e"},{"cell_type":"code","source":["# Function to automate whitespace removal across the entire DataFrame\n","def trim_all_string_cols(df):\n","    \"\"\"\n","    Identifies all string-type columns and applies the SQL trim function to remove \n","    leading and trailing whitespace.\n","    \n","    Args:\n","        df (pyspark.sql.DataFrame): The input DataFrame to be cleaned.\n","        \n","    Returns:\n","        pyspark.sql.DataFrame: A new DataFrame with trimmed string values.\n","    \"\"\"    \n","    # Get all columns that are of type 'string'\n","    string_columns = [c for c, t in df.dtypes if t == 'string']\n","\n","    # Apply trim to those columns specifically\n","    df_trimmed = df.withColumns({c: trim(col(c)) for c in string_columns})\n","\n","    # Return the transformed DataFrame with cleaned string data\n","    return df_trimmed"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:00.8019932Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:00.8331834Z","execution_finish_time":"2026-02-01T02:20:01.1833397Z","parent_msg_id":"1956fa4a-93f2-4f7b-b28e-dfd3ad36f5ed"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d910dd6-af83-4693-95be-68b0986260c6"},{"cell_type":"code","source":["# Wrapper function to orchestrate data cleaning tasks\n","def trim_data(df):\n","\n","    trimmed_df = trim_all_string_cols(df)\n","\n","    return trimmed_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:01.5774342Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:01.5785931Z","execution_finish_time":"2026-02-01T02:20:01.9184314Z","parent_msg_id":"8a4933d9-7131-4afe-9ec0-d405cbbce8fd"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c76b6d4-8d9d-432c-8a3e-87dcf202e484"},{"cell_type":"code","source":["# Function to persist DataFrame results back to the Lakehouse as a Delta Table\n","def load_df_to_delta(df, table_name: str, mode: str = \"overwrite\"):\n","    \"\"\"\n","    Load and saves dataframe data as a Delta Table.\n","    \n","    Args:\n","        df (spark DataFrame): df to be loaded to delta table.\n","        table_name: name of destination table \n","        mode (str): 'overwrite' to replace the table, default.\n","    \"\"\"\n","    try:\n","        # 1. Save as a Delta Table \n","        df.write.format(\"delta\") \\\n","            .mode(mode) \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","\n","        print(f\"Table '{table_name}' loaded successfully !\")\n","        \n","    except Exception as e:\n","        print(f\"Error processing table {table_name}: {str(e)}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:02.5471619Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:02.5483631Z","execution_finish_time":"2026-02-01T02:20:02.8963871Z","parent_msg_id":"13748d3d-0eb9-4dd3-98ae-ca110a03dcca"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ff4a38cd-06db-4323-bc09-8c97e2895e7a"},{"cell_type":"code","source":["# Function to perform Data Quality (DQ) checks for null values in key customer fields\n","def check_null_columns(df):\n","    \"\"\"\n","    Scans specific customer identity and location columns for NULL values,\n","    counts the occurrences, and previews the problematic records.\n","    \n","    Args:\n","        df (pyspark.sql.DataFrame): The DataFrame to be inspected.\n","    \"\"\"\n","\n","    # Identify rows where any of the mandatory customer fields are null.\n","    null_df = df.filter(\n","            F.col(\"customer_id\").isNull() | \n","            F.col(\"customer_unique_id\").isNull() | \n","            F.col(\"customer_zip_code_prefix\").isNull() | \n","            F.col(\"customer_city\").isNull() |             \n","            F.col(\"customer_state\").isNull()\n","        )\n","\n","    null_count = null_df.count()\n","    print(f\"Records with null columns: {null_count}\")\n","\n","    if null_count > 0:\n","        # Reuses the 'null_df' transformation logic to output rows to the console\n","        null_df.show()\n","pass"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:03.4824967Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:03.4836965Z","execution_finish_time":"2026-02-01T02:20:03.8244307Z","parent_msg_id":"2e2831a0-6dcf-4c90-8529-bdd2c566494d"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8204c3a8-0cc1-430c-8942-397237e55e6f"},{"cell_type":"code","source":["# Function acting as a central hub for data cleaning and filtering logic\n","def clean_dirty_data(df):\n","    #Clean by removing dirty data\n","\n","    print(\"Cleaning script executed ! \")\n","\n","    return df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:04.6043735Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:04.605596Z","execution_finish_time":"2026-02-01T02:20:04.931694Z","parent_msg_id":"9ab85e79-7bbd-41eb-adaa-01633cd91990"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 20, Finished, Available, Finished)"},"metadata":{}}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32bbec80-f1cc-46ec-bf1a-97d6a08fdd72"},{"cell_type":"code","source":["# Function to validate foreign key relationships between two tables\n","def check_referencing_key(df, ref_table_name: str, df_col: str, ref_col: str, error_col: str): \n","    \"\"\"\n","    Validates that keys in the input DataFrame exist in a reference Spark table.\n","    If keys are missing, it flags those rows in a new error column.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The source DataFrame to validate.\n","        ref_table_name (str): The name of the Catalog table used for validation.\n","        df_col (str): The join column name in the source DataFrame.\n","        ref_col (str): The join column name in the reference table.\n","        error_col (str): The name of the column to create for flagging errors (e.g., 'is_invalid').\n","\n","    Returns:\n","        pyspark.sql.DataFrame: The DataFrame with an additional error flag column if mismatches exist.\n","    \"\"\"\n","\n","    # 1. Filter for rows in df where the zip_code IS NOT in referencing_df\n","    # For this compare the desired column to keep the join lean\n","    ref_df = spark.read.table(ref_table_name)\n","\n","    # 2. Perform a Left Anti-style check using a Left Join\n","    invalid_keys_df = df.alias(\"main\").join(\n","        ref_df.alias(\"ref\"), \n","        df[df_col] == ref_df[ref_col], \n","        how=\"left\"\n","    )\n","\n","    # 3. Identify rows where the join failed (resulted in NULL from the reference side)\n","    invalid_count = invalid_keys_df.filter(col(\"ref.\" + ref_col).isNull()).count()\n","\n","    if invalid_count > 0:\n","        print(f\"Found {invalid_count} Keys: {df_col} not present in the referencing table: {ref_table_name}\")\n","        print(f\"{error_col} column set to Y\")\n","\n","        # Display the specific keys that are missing from the reference table\n","        invalid_keys_df.select(df_col).filter(col(\"ref.\" + ref_col).isNull()).show()\n","\n","        # 4. Add the error flag ('Y') and clean up the schema\n","        # We use F.when() to flag the null matches and select \"main.*\" to remove ref columns.\n","        df = invalid_keys_df.withColumn(\n","        error_col,\n","        F.when(F.col(\"ref.\" + ref_col).isNull(), \"Y\")\n","        ).select(\"main.*\", error_col)\n","\n","    else:\n","        # Confirms all data is valid according to the reference table\n","        print(f\"All {df_col} found to be valid and present in the referencing table: {ref_table_name}.\\n\")\n","\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:05.7145873Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:05.7158492Z","execution_finish_time":"2026-02-01T02:20:05.9920974Z","parent_msg_id":"28cd2ce0-051e-40fe-82d0-fec950125c00"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 21, Finished, Available, Finished)"},"metadata":{}}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cf859afe-03b5-4e43-ad65-79e7b6be5947"},{"cell_type":"code","source":["# Function to verify data uniqueness and identify duplicate primary keys\n","def check_records_count(df, col_name: str): \n","    \"\"\"\n","    Validates the integrity of a specific column by comparing total row counts \n","    against distinct row counts and identifying any duplicate values.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The DataFrame to be audited.\n","        col_name (str): The name of the column that should be unique (e.g., a Primary Key).\n","    \"\"\"\n","\n","    # Print count records and print (if any duplicate records)\n","    print(f\"Total count: {df.select(col_name).count()}\")\n","    print(f\"Distinct count: {df.select(col_name).distinct().count()}\")\n","    print(\"Records to investigate (if any not unique key)\")\n","    df.groupBy(col_name).count().filter(\"count > 1\").show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:06.5553983Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:06.5565787Z","execution_finish_time":"2026-02-01T02:20:06.8571032Z","parent_msg_id":"07d7ab81-50d2-4f2c-a70f-44bb3189de03"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":20,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ac5d5bb-0fe1-4fad-9cdf-98f31f1f32bc"},{"cell_type":"code","source":["# ==============================================================================\n","# MAIN ETL PIPELINE EXECUTION: Bronze to Silver\n","# ==============================================================================\n","\n","# 1. Configuration: Define source and destination entry points in the Lakehouse\n","from_table_name = 'BronzeLakeHouse.dbo.customers_bronze'\n","to_table_name = 'customers_silver'\n","\n","# 2. Extract: Load the raw data from the Bronze layer into a Spark DataFrame\n","df = extract_table(from_table_name)\n","\n","# 3. Transform (Cleaning): Remove whitespace from all string columns\n","df = trim_data(df)\n","\n","# 4. Data Quality Check: Print count of rows with null values in mandatory fields\n","check_null_columns(df)\n","\n","# 5. Referential Integrity Check A: Validate customer zip codes against geolocation master data\n","df = check_referencing_key(df, 'geolocation_silver', 'customer_zip_code_prefix', 'geolocation_zip_code_prefix', 'not_in_geo_flag')\n","\n","# 6. Referential Integrity Check B: Validate customer IDs against the orders table\n","df = check_referencing_key(df, 'BronzeLakeHouse.dbo.orders_bronze', 'customer_id', 'customer_id', 'not_in_orders_flag')\n","\n","# 7. Uniqueness Validation: Ensure 'customer_id' remains a unique primary key after transformations\n","check_records_count(df, 'customer_id')\n","\n","# 8. Conditional Cleaning: Optional hook to remove rows flagged in steps 5 & 6\n","# >> If dirty Record to Clean here. \n","# df = clean_dirty_data(df)\n","\n","# 9. Load: Persist the final validated DataFrame as a Delta table in the Silver layer\n","load_df_to_delta(df, to_table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":23,"statement_ids":[23],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:07.5672382Z","session_start_time":null,"execution_start_time":"2026-02-01T02:20:07.5684487Z","execution_finish_time":"2026-02-01T02:21:20.5776211Z","parent_msg_id":"4a9793b7-8cd8-476a-9ed8-934370cb3c7d"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 23, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Records with null columns: 0\nFound 278 Keys: customer_zip_code_prefix not present in the referencing table: geolocation_silver\nnot_in_geo_flag column set to Y\n+------------------------+\n|customer_zip_code_prefix|\n+------------------------+\n|                   72300|\n|                   11547|\n|                   64605|\n|                   72465|\n|                   07729|\n|                   72904|\n|                   35408|\n|                   78554|\n|                   73369|\n|                   08980|\n|                   29949|\n|                   65137|\n|                   28655|\n|                   73255|\n|                   28388|\n|                   06930|\n|                   71676|\n|                   64047|\n|                   29949|\n|                   61906|\n+------------------------+\nonly showing top 20 rows\n\nAll customer_id found to be valid and present in the referencing table: BronzeLakeHouse.dbo.orders_bronze.\n\nTotal count: 99441\nDistinct count: 99441\nRecords to investigate (if any not unique key)\n+-----------+-----+\n|customer_id|count|\n+-----------+-----+\n+-----------+-----+\n\nTable 'customers_silver' loaded successfully !\n"]}],"execution_count":21,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4b50c8ea-fca9-4106-b778-9cd78c4cb0a7"},{"cell_type":"code","source":["# df.createOrReplaceTempView(\"test\")\n","# # check_df = spark.sql(\"\"\" select count(*) from test where not_in_geo_flag IS NULL \"\"\")\n","# check_df = spark.sql(\"\"\" select count(*) from test where not_in_orders_flag IS NULL \"\"\")\n","\n","# print(check_df.show())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":24,"statement_ids":[24],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:08.1768083Z","session_start_time":null,"execution_start_time":"2026-02-01T02:21:20.5806039Z","execution_finish_time":"2026-02-01T02:21:20.9263704Z","parent_msg_id":"24b52d74-0ae5-4666-9077-d7f826779ea1"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 24, Finished, Available, Finished)"},"metadata":{}}],"execution_count":22,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a2be470-96f8-4267-bebd-3393e4f0de90"},{"cell_type":"code","source":["# stop spark session\n","mssparkutils.session.stop()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":25,"statement_ids":[25],"state":"finished","livy_statement_state":"available","session_id":"0d808e92-f23c-4de3-b09a-add80c9445c0","normalized_state":"finished","queued_time":"2026-02-01T02:20:08.7469752Z","session_start_time":null,"execution_start_time":"2026-02-01T02:21:20.9284061Z","execution_finish_time":"2026-02-01T02:21:22.4404876Z","parent_msg_id":"df5c9801-19dc-4c89-b21c-d22733487ea1"},"text/plain":"StatementMeta(, 0d808e92-f23c-4de3-b09a-add80c9445c0, 25, Finished, Available, Finished)"},"metadata":{}}],"execution_count":23,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a98609d6-d52e-4001-b1b2-df5eb74236d1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"791a60f4-f8f8-4ea5-9519-0426798180f4"},{"id":"dc1b06fe-c8d2-48ee-9a55-d0149ad78dcd"}],"default_lakehouse":"dc1b06fe-c8d2-48ee-9a55-d0149ad78dcd","default_lakehouse_name":"SilverLakeHouse","default_lakehouse_workspace_id":"7d303a82-1fe7-4f4b-82d6-d23015b8c472"}}},"nbformat":4,"nbformat_minor":5}