{"cells":[{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d47025bb-7a21-4a59-bbc7-17f7bf11b2ab"},{"cell_type":"code","source":["# ==============================================================================\n","# STANDARD LIBRARIES & SPARK CONFIGURATION\n","# ==============================================================================\n","\n","# mssparkutils: Microsoft Fabric utility toolset used for file system \n","# management, secret retrieval, and cross-notebook orchestration.\n","from notebookutils import mssparkutils\n","\n","# col: Function used to select and wrap column names for Spark transformations.\n","# trim: Removes leading and trailing whitespace from string-type data.\n","from pyspark.sql.functions import col, trim\n","\n","# F: Standard alias for PySpark SQL functions. Using this namespace ensures \n","# code clarity and prevents conflicts with built-in Python functions.\n","from pyspark.sql import functions as F"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:24.7285517Z","session_start_time":"2026-02-01T03:27:24.7297137Z","execution_start_time":"2026-02-01T03:27:45.6513342Z","execution_finish_time":"2026-02-01T03:27:46.0169763Z","parent_msg_id":"8a20b525-e759-4c5b-83c9-40a03e59f69a"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cec0f50e-246a-4805-95d3-9a7943762c2a"},{"cell_type":"code","source":["# Function to interface with the Spark catalog and retrieve data\n","def extract_table(table_name: str):\n","    \"\"\"\n","    Reads a table from the Spark catalog and loads it into a DataFrame.\n","    \n","    In Microsoft Fabric, this pulls metadata and data from the 'Tables' \n","    section of your Lakehouse.\n","\n","    Args:\n","        table_name (str): The name of the table to be extracted.\n","                          Can be a simple name ('customers') or a \n","                          three-part name ('Lakehouse.dbo.customers').\n","\n","    Returns:\n","        pyspark.sql.dataframe.DataFrame: A Spark DataFrame containing the table data.\n","    \"\"\"\n","    df = spark.read.table(table_name)\n","    \n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:24.9632004Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:46.0191732Z","execution_finish_time":"2026-02-01T03:27:46.3688933Z","parent_msg_id":"c87c7e19-8b27-493f-ae3e-08fb07e01baa"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6dc86ea-26f4-4591-bffc-8935f4f83b0e"},{"cell_type":"code","source":["# Function to automate whitespace removal across the entire DataFrame\n","def trim_all_string_cols(df):\n","    \"\"\"\n","    Identifies all string-type columns and applies the SQL trim function to remove \n","    leading and trailing whitespace.\n","    \n","    Args:\n","        df (pyspark.sql.DataFrame): The input DataFrame to be cleaned.\n","        \n","    Returns:\n","        pyspark.sql.DataFrame: A new DataFrame with trimmed string values.\n","    \"\"\"\n","    \n","    # List comprehension to filter columns: df.dtypes returns a list of (name, type) tuples.\n","    # We only target columns where the type 't' is exactly 'string'.\n","    string_columns = [c for c, t in df.dtypes if t == 'string']\n","\n","    # withColumns (introduced in Spark 3.3+) allows for multiple column updates at once.\n","    # This dictionary comprehension maps each string column to a trimmed version of itself.\n","    df_trimmed = df.withColumns({c: trim(col(c)) for c in string_columns})\n","\n","    # Return the transformed DataFrame with cleaned string data\n","    return df_trimmed"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:25.4446652Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:46.3711689Z","execution_finish_time":"2026-02-01T03:27:46.7290316Z","parent_msg_id":"d42cff2b-c108-46cb-8ebe-e91d67ca576b"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d910dd6-af83-4693-95be-68b0986260c6"},{"cell_type":"code","source":["# Wrapper function to orchestrate the initial transformation phase\n","def trim_data(df):\n","\n","    trimmed_df = trim_all_string_cols(df)\n","    # group_df = trimmed_df.groupBy(['geolocation_zip_code_prefix', 'geolocation_state']) \\\n","    #     .agg({'geolocation_lat': 'mean', 'geolocation_lng':'mean'})\n","\n","    return trimmed_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:25.6991713Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:46.7311627Z","execution_finish_time":"2026-02-01T03:27:47.1330048Z","parent_msg_id":"a6c237af-9614-4d85-bbe9-71c1bc923ffd"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c76b6d4-8d9d-432c-8a3e-87dcf202e484"},{"cell_type":"code","source":["# Function to persist DataFrame results back to the Lakehouse as a Delta Table\n","def load_df_to_delta(df, table_name: str, mode: str = \"overwrite\"):    \n","    \"\"\"\n","    Load and saves dataframe data as a Delta Table.\n","    \n","    Args:\n","        df (spark DataFrame): df to be loaded to delta table.\n","        table_name: name of destination table \n","        mode (str): 'overwrite' to replace the table, default.\n","    \"\"\"\n","    try:\n","        # 1. Save as a Delta Table \n","        df.write.format(\"delta\") \\\n","            .mode(mode) \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(table_name)\n","\n","        print(f\"Table '{table_name}' loaded successfully !\")\n","        \n","    except Exception as e:\n","        print(f\"Error processing table {table_name}: {str(e)}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:25.9953166Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:47.1352958Z","execution_finish_time":"2026-02-01T03:27:47.4852307Z","parent_msg_id":"e43d083e-8ee8-4e3c-a5a0-d4921b4f200b"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ff4a38cd-06db-4323-bc09-8c97e2895e7a"},{"cell_type":"code","source":["# Function to identify and display records with missing mandatory product data\n","def check_null_columns(df):\n","    \"\"\"\n","    Scans the DataFrame for NULL values in the product_id column,\n","    calculates the impact, and previews the records for investigation.\n","    \n","    Args:\n","        df (pyspark.sql.DataFrame): The DataFrame to be inspected.\n","    \"\"\"\n","\n","    # Filter for rows where critical identifiers are missing.\n","    null_df = df.filter(       \n","            F.col(\"product_id\").isNull()\n","        )\n","    # Perform a count action to determine the scale of data quality issues\n","    null_count = null_df.count()\n","    print(f\"Records with null columns: {null_count}\")\n","\n","    # If gaps are found, display the problematic rows for manual audit\n","    if null_count > 0:\n","            # Reuses the filtered logic\n","        null_df.show()\n","pass"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:26.2116615Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:47.487387Z","execution_finish_time":"2026-02-01T03:27:47.8192524Z","parent_msg_id":"4a683536-8639-4cc7-bc96-1bfc3fdebd71"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8204c3a8-0cc1-430c-8942-397237e55e6f"},{"cell_type":"code","source":["# Function acting as a central hub for specific data removal or filtering logic\n","def clean_dirty_data(df):\n","    \"\"\"\n","    Applies custom business rules to strictly remove 'dirty' or invalid records \n","    from the dataset. Currently acts as a pass-through function.\n","    \"\"\"\n","\n","    #Clean by removing dirty data\n","    print(\"Cleaning script executed ! \")\n","\n","    return df\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:26.4873539Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:47.8213945Z","execution_finish_time":"2026-02-01T03:27:48.1344711Z","parent_msg_id":"a924f34b-3842-445b-b7b2-e50cc23e2cea"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32bbec80-f1cc-46ec-bf1a-97d6a08fdd72"},{"cell_type":"code","source":["# Function to validate foreign key relationships and flag orphaned records\n","def check_referencing_key(df, ref_table_name: str, df_col: str, ref_col: str, error_col: str): \n","    \"\"\"\n","    Performs a left join against a reference table to find keys in the source \n","    that do not exist in the reference. Orphaned rows are flagged with 'Y'.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The source DataFrame to validate.\n","        ref_table_name (str): The name of the reference catalog table.\n","        df_col (str): The column name in the source DataFrame to check.\n","        ref_col (str): The column name in the reference table to match against.\n","        error_col (str): The name of the flag column to be created.\n","\n","    Returns:\n","        pyspark.sql.DataFrame: The DataFrame with the added error flag column.\n","    \"\"\"\n","\n","    ## 1. Load the reference data from the Lakehouse catalog\n","    ref_df = spark.read.table(ref_table_name)\n","\n","    # 2. Left Join: Keeps all records from 'main' and matches from 'ref'\n","    # Aliasing prevents naming collisions between the two tables.\n","    invalid_keys_df = df.alias(\"main\").join(\n","        ref_df.alias(\"ref\"), \n","        df[df_col] == ref_df[ref_col], \n","        how=\"left\"\n","    )\n","    \n","    # 3. Identify mismatches where the reference column is NULL\n","    invalid_count = invalid_keys_df.filter(col(\"ref.\" + ref_col).isNull()).count()\n","\n","    if invalid_count > 0:\n","        print(f\"Found {invalid_count} Keys: {df_col} not present in the referencing table: {ref_table_name}\")\n","        print(f\"{error_col} column set to Y\")\n","\n","        # Display the specific raw values that failed the lookup\n","        invalid_keys_df.select(col(\"main.\" + df_col)) \\\n","                       .filter(col(\"ref.\" + ref_col) \\\n","                       .isNull()).show()\n","\n","        df = invalid_keys_df.withColumn(\n","            error_col,\n","            F.when(F.col(\"ref.\" + ref_col).isNull(), \"Y\")\n","            ).select(\"main.*\", error_col)\n","        \n","        # invalid_keys_df.filter(col(\"ref.\" + ref_col).isNull()) \\\n","        #                .groupBy(col(\"main.\" + df_col)) \\\n","        #                .count().show()             \n","    else:\n","        print(f\"All {df_col} found to be valid and present in the referencing table: {ref_table_name}.\\n\")\n","\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:26.7930577Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:48.1369292Z","execution_finish_time":"2026-02-01T03:27:48.4412775Z","parent_msg_id":"b4184e2c-c7e9-428b-955d-d2c154ae2c59"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cf859afe-03b5-4e43-ad65-79e7b6be5947"},{"cell_type":"code","source":["# Function to verify data uniqueness and identify duplicate primary keys\n","def check_records_count(df, col_name: str): \n","# Check that after cleaning records is now unique and dirty data removed\n","\n","    print(f\"Total count: {df.select(col_name).count()}\")\n","    print(f\"Distinct count: {df.select(col_name).distinct().count()}\")\n","    print(\"Records to investigate (if any not unique key)\")\n","    df.groupBy(col_name).count().filter(\"count > 1\").show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:27.0270557Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:48.4434303Z","execution_finish_time":"2026-02-01T03:27:48.8520492Z","parent_msg_id":"5b819680-95c6-4fb9-b5e7-dc7302fd5f6d"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ac5d5bb-0fe1-4fad-9cdf-98f31f1f32bc"},{"cell_type":"code","source":["# Function to standardize data types and correct column naming conventions\n","def convert_data_type_rename_column(df):\n","    \"\"\"\n","    Standardizes the schema of the products dataset by casting numerical strings \n","    to appropriate Float/Integer types and correcting spelling errors in headers.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The input DataFrame from the Bronze layer.\n","\n","    Returns:\n","        pyspark.sql.DataFrame: A DataFrame with corrected types and renamed columns.\n","    \"\"\"\n","\n","    # 1. Type Casting: Converting metadata and physical dimensions to numerical types.\n","    # We use .cast(\"float\") for measurements and .cast(\"int\") for discrete counts.\n","    df_convert = df.withColumn(\"product_name_lenght\", col(\"product_name_lenght\").cast(\"float\")) \\\n","                       .withColumn(\"product_description_lenght\", col(\"product_description_lenght\").cast(\"float\")) \\\n","                       .withColumn(\"product_photos_qty\", col(\"product_photos_qty\").cast(\"int\")) \\\n","                       .withColumn(\"product_weight_g\", col(\"product_weight_g\").cast(\"float\")) \\\n","                       .withColumn(\"product_length_cm\", col(\"product_length_cm\").cast(\"float\")) \\\n","                       .withColumn(\"product_height_cm\", col(\"product_height_cm\").cast(\"float\")) \\\n","                       .withColumn(\"product_width_cm\", col(\"product_width_cm\").cast(\"float\")) \n","\n","    # 2. Renaming: Correcting the spelling of 'lenght' to 'length'.\n","    # This ensures the Silver layer adheres to standard English naming conventions.\n","    df_renamed = df_convert.withColumnRenamed(\"product_name_lenght\", \"product_name_length\") \\\n","                           .withColumnRenamed(\"product_description_lenght\", \"product_description_length\")\n","\n","    return df_renamed\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:27.2310115Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:48.8543004Z","execution_finish_time":"2026-02-01T03:27:49.1260837Z","parent_msg_id":"7bb8d894-9dc1-4621-98fb-c3dea6628518"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4820b27d-20ae-429c-a6ee-6ce7c23b6b8c"},{"cell_type":"code","source":["# Function to enrich the dataset with English category translations\n","def merge_english_translation_column(df):\n","    \"\"\"\n","    Joins the product data with a translation reference table to provide \n","    English equivalents for Portuguese category names.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The product DataFrame.\n","\n","    Returns:\n","        pyspark.sql.DataFrame: The enriched DataFrame including English translations.\n","    \"\"\"\n","\n","    # 1. Extract and Clean the Translation reference table\n","    translate_df = extract_table(\"BronzeLakeHouse.dbo.product_category_name_translation_bronze\")\n","    translate_df = trim_data(translate_df)\n","\n","    # 2. Collision Prevention: Suffix all columns in the reference table with '_ref'.\n","    # This ensures that joining doesn't result in ambiguous column names.\n","    translate_df = translate_df.select([col(c).alias(c + \"_ref\") for c in translate_df.columns])\n","\n","    # 3. Perform Left Join to pull in the English translation\n","    # We select all original columns (\"main.*\") plus only the specific translation column.\n","    merge_df = df.alias(\"main\").join(\n","        translate_df.alias(\"ref\"), \n","        df['product_category_name'] == translate_df['product_category_name_ref'], \n","        how=\"left\"\n","    ).select(\"main.*\", \"product_category_name_english_ref\")\n","\n","    # 4. Data Cleanup: Drop the redundant/untransformed length columns.\n","    # (Note: These are usually dropped here because they were renamed in the previous step).\n","    df = merge_df.drop(\"product_name_lenght\", \"product_description_lenght\")    \n","    \n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:27.5028899Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:49.1286768Z","execution_finish_time":"2026-02-01T03:27:49.4846392Z","parent_msg_id":"8c342532-8bed-4a83-a63b-c25ebe50e3d8"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"43d71dab-58d7-43a4-b06f-876d512314d2"},{"cell_type":"code","source":["# Function to safely cast columns to Float while auditing for non-numeric noise\n","def cast_to_float(df, column_list):\n","    \"\"\"\n","    Iterates through a list of columns, identifies non-numeric strings that would \n","    fail a float conversion, and then performs the type cast.\n","\n","    Args:\n","        df (pyspark.sql.DataFrame): The input DataFrame.\n","        column_list (list): A list of column names to be converted to Float.\n","\n","    Returns:\n","        pyspark.sql.DataFrame: The DataFrame with columns cast to Float type.\n","    \"\"\"\n","\n","    for c in column_list:\n","\n","        # Regex pattern: Matches optional negative sign, optional leading digits, \n","        # an optional decimal point, and optional trailing digits.\n","        numeric_pattern = r'^-?\\d*\\.?\\d*$'\n","        \n","        # 1. Audit Phase: Identify \"dirty\" data.\n","        # Finds rows that do not match the numeric pattern and are not already NULL.\n","        invalid_count = df.filter(~col(c).rlike(numeric_pattern) & col(c).isNotNull()).count()\n","        \n","        if invalid_count > 0:\n","            # Alerts the user to data quality issues that will result in NULLs after casting.\n","            print(f\"Warning: Column '{c}' has {invalid_count} non-numeric rows. These will become NULL.\")\n","        \n","        # 2. Transformation Phase: Apply the cast.\n","        df = df.withColumn(c, col(c).cast(\"float\"))\n","        \n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:27.6703486Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:49.4867848Z","execution_finish_time":"2026-02-01T03:27:49.7886055Z","parent_msg_id":"5bad3aa0-3f09-40f5-b623-c91cee72685b"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f802782b-88a6-4956-9af0-869b83982b92"},{"cell_type":"code","source":["# ==============================================================================\n","# MAIN ETL PIPELINE EXECUTION: Products Bronze to Silver\n","# ==============================================================================\n","\n","# 1. Configuration: Define source and destination\n","from_table_name = 'BronzeLakeHouse.dbo.products_bronze'\n","to_table_name = 'products_silver'\n","\n","# 2. Extract: Load raw product data\n","df = extract_table(from_table_name)\n","\n","# 3. Transform (Cleaning): Global trim on string columns and DQ Null check\n","df = trim_data(df)\n","check_null_columns(df)\n","\n","# 4. Referential Integrity: Validate categories against the translation reference\n","# Adds 'not_in_product_translate_flag' for categories missing translation mapping.\n","df = check_referencing_key(df, 'BronzeLakeHouse.dbo.product_category_name_translation_bronze', \\\n","                               'product_category_name', \\\n","                               'product_category_name', \\\n","                               'not_in_product_translate_flag')\n","\n","# 5. Enrichment: Merge English category names into the main dataset\n","df = merge_english_translation_column(df)\n","\n","# 6. Schema Enforcement: Robustly cast measurement columns to Float\n","# This step includes regex auditing to warn about non-numeric noise.\n","columns_to_convert = [\"product_photos_qty\", \"product_weight_g\",\"product_length_cm\",\"product_height_cm\",\"product_width_cm\"]\n","df = cast_to_float(df, columns_to_convert)\n","\n","# 7. Print the resulting schema and check for unique primary keys\n","df.printSchema(1)\n","check_records_count(df, 'product_id')\n","\n","# 8. Conditional Cleaning: Optional hook for final data removal logic\n","# df = clean_dirty_data(df)\n","\n","# 9. Load: Persist the enriched, typed, and validated data to the Silver layer\n","load_df_to_delta(df, to_table_name)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:27.8647544Z","session_start_time":null,"execution_start_time":"2026-02-01T03:27:49.7907716Z","execution_finish_time":"2026-02-01T03:28:51.6762432Z","parent_msg_id":"830ae16a-65cc-421d-aec7-3d5269c09b5a"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Records with null columns: 0\nFound 623 Keys: product_category_name not present in the referencing table: BronzeLakeHouse.dbo.product_category_name_translation_bronze\nnot_in_product_translate_flag column set to Y\n+---------------------+\n|product_category_name|\n+---------------------+\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n|                 NULL|\n+---------------------+\nonly showing top 20 rows\n\nroot\n |-- product_id: string (nullable = true)\n |-- product_category_name: string (nullable = true)\n |-- product_photos_qty: float (nullable = true)\n |-- product_weight_g: float (nullable = true)\n |-- product_length_cm: float (nullable = true)\n |-- product_height_cm: float (nullable = true)\n |-- product_width_cm: float (nullable = true)\n |-- not_in_product_translate_flag: string (nullable = true)\n |-- product_category_name_english_ref: string (nullable = true)\n\nTotal count: 32951\nDistinct count: 32951\nRecords to investigate (if any not unique key)\n+----------+-----+\n|product_id|count|\n+----------+-----+\n+----------+-----+\n\nTable 'products_silver' loaded successfully !\n"]}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4b50c8ea-fca9-4106-b778-9cd78c4cb0a7"},{"cell_type":"code","source":["# df.createOrReplaceTempView(\"test\")\n","# check_df = spark.sql(\"\"\" select count(*) from test where not_in_geo_flag = 'Y' \"\"\")\n","# # check_df = spark.sql(\"\"\" select count(*) from test where not_in_orders_flag IS NULL \"\"\")\n","\n","# check_df.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:28.1113571Z","session_start_time":null,"execution_start_time":"2026-02-01T03:28:51.678854Z","execution_finish_time":"2026-02-01T03:28:51.9897953Z","parent_msg_id":"95131823-914f-4206-973b-7e9cc0f889b2"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a2be470-96f8-4267-bebd-3393e4f0de90"},{"cell_type":"code","source":["# ==============================================================================\n","# SESSION TERMINATION\n","# ==============================================================================\n","\n","# Explicitly stops the Spark session and releases the allocated compute \n","# resources (executors/nodes) back to the Microsoft Fabric pool.\n","\n","mssparkutils.session.stop()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24","normalized_state":"finished","queued_time":"2026-02-01T03:27:28.2872394Z","session_start_time":null,"execution_start_time":"2026-02-01T03:28:51.9919961Z","execution_finish_time":"2026-02-01T03:28:53.4908908Z","parent_msg_id":"e8260b5f-6972-4280-8ae4-56948eca0049"},"text/plain":"StatementMeta(, f0bd5a0c-d5b0-46e4-a6d0-b8b4e9006b24, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a98609d6-d52e-4001-b1b2-df5eb74236d1"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"dc1b06fe-c8d2-48ee-9a55-d0149ad78dcd"},{"id":"791a60f4-f8f8-4ea5-9519-0426798180f4"}],"default_lakehouse":"dc1b06fe-c8d2-48ee-9a55-d0149ad78dcd","default_lakehouse_name":"SilverLakeHouse","default_lakehouse_workspace_id":"7d303a82-1fe7-4f4b-82d6-d23015b8c472"}}},"nbformat":4,"nbformat_minor":5}