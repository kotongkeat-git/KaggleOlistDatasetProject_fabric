{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb326d0-10b8-4455-b4d5-a3969f7db927",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### <font color='blue'>**Download Kaggle Datasets** | **Load to LakeHouse (Bronze)**</font>\n",
    "Total of 9 datasets from https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "1. olist_customers_dataset.csv\n",
    "2. olist_geolocation_dataset.csv\n",
    "3. olist_order_items_dataset.csv\n",
    "4. olist_order_payments_dataset.csv\n",
    "5. olist_order_reviews_dataset.csv\n",
    "6. olist_orders_dataset.csv\n",
    "7. olist_products_dataset.csv\n",
    "8. olist_sellers_dataset.csv\n",
    "9. product_category_name_translation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f92b5-628a-4dfe-bd02-f733d406ce0c",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### <font color='blue'> **Install Kaggle Library** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "824e9071-fe70-4ac6-840a-94ac6c454f38",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:39:08.7477406Z",
       "execution_start_time": "2026-02-06T01:38:46.8347405Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a40526f6-a9fe-4c93-87f6-d6875880f3ef",
       "queued_time": "2026-02-06T01:38:30.1446806Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": "2026-02-06T01:38:30.1464367Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        3,
        4,
        5,
        6,
        7,
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaggle\n",
      "  Downloading kaggle-1.8.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: bleach in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (4.1.0)\n",
      "Collecting kagglesdk<1.0,>=0.1.15 (from kaggle)\n",
      "  Downloading kagglesdk-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (23.1)\n",
      "Requirement already satisfied: protobuf in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (3.20.3)\n",
      "Requirement already satisfied: python-dateutil in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: requests in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from kaggle) (2.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from bleach->kaggle) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests->kaggle) (2024.2.2)\n",
      "Downloading kaggle-1.8.4-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kagglesdk-0.1.15-py3-none-any.whl (160 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kagglesdk, kaggle\n",
      "Successfully installed kaggle-1.8.4 kagglesdk-0.1.15\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Warning: PySpark kernel has been restarted to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the Kaggle library\n",
    "# 1. Used for authenticating with Kaggle's public API.\n",
    "# 2. Allows programmatic download of datasets (e.g., Olist E-commerce).\n",
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54b5957-a618-4f93-8728-d3f8d31c6b27",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <font color='blue'>**Import Libraries** </font>\n",
    "\n",
    "| Import Statement | Purpose |\n",
    "| :--- | :--- |\n",
    "| `import os` | Used to talk to the computer's operating system. It lets the notebook do things like checking file paths and creating folders where the data will be saved.|\n",
    "| `from kaggle.api.kaggle_api_extended import KaggleApi` | Imports the specific tool needed to connect to Kaggle. This object handles authentication and allows us to run commands (like downloading datasets) directly from the notebook. |\n",
    "| `from notebookutils import mssparkutils` | Provides a suite of utilities for Microsoft Fabric, used for file system management (OneLake), notebook orchestration, and credential handling. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42648330-9575-476b-b6e0-e60f5a2a668e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:39:14.083936Z",
       "execution_start_time": "2026-02-06T01:39:13.2518135Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "abdd58a7-5b9a-4b17-b616-b80ef4e6f447",
       "queued_time": "2026-02-06T01:38:30.4209172Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Welcome to your new notebook\n",
    "# Type here in the cell editor to add code!\n",
    "\n",
    "import os\n",
    "\n",
    "# Set credentials (you can also use Fabric Environment Variables for better security)\n",
    "os.environ['KAGGLE_USERNAME'] = your_actual_username\n",
    "os.environ['KAGGLE_KEY'] = your_actual_key\n",
    "\n",
    "# Standard Libraries\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Define path (Fabric notebooks mount the Lakehouse at /lakehouse/default/)\n",
    "# download_path = \"/lakehouse/default/Files/KaggleData/\"\n",
    "\n",
    "# Download and unzip\n",
    "# api.dataset_download_files('olistbr/brazilian-ecommerce', path=download_path, unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcba6877-d21b-4359-b104-aae377089bf1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:39:14.3751846Z",
       "execution_start_time": "2026-02-06T01:39:14.0864709Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "61f65116-4ec6-4a02-9b4f-caf4279448b0",
       "queued_time": "2026-02-06T01:38:30.8532817Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_kaggle_dataset(dataset_name: str, local_path: str):\n",
    "    \"\"\"\n",
    "    Authenticates with Kaggle and downloads/unzips a dataset to a specific Fabric directory.\n",
    "   . \n",
    "    Args:\n",
    "        dataset_name (str): The Kaggle dataset string (e.g., 'olistbr/brazilian-ecommerce').\n",
    "        local_path (str): The destination path in the Fabric Lakehouse\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Initialize the Kaggle API client and look for credentials (kaggle.json)\n",
    "        # Typically looks in ~/.kaggle/ or environment variables KAGGLE_USERNAME/KAGGLE_KEY\n",
    "        api = KaggleApi()\n",
    "        api.authenticate()\n",
    "        \n",
    "        # Ensure the directory exists\n",
    "        if not os.path.exists(local_path):\n",
    "            os.makedirs(local_path)\n",
    "            print(f\"Created directory: {local_path}\")\n",
    "\n",
    "        print(f\"Downloading dataset '{dataset_name}' to {local_path}...\")\n",
    "        \n",
    "        # Download and unzip\n",
    "        api.dataset_download_files(dataset_name, path=local_path, unzip=True)\n",
    "        \n",
    "        print(\"Download and extraction complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f962581b-8814-4612-8d4d-7465c1550ad2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:39:14.6478648Z",
       "execution_start_time": "2026-02-06T01:39:14.3773312Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6bf2a954-ee03-491b-8b32-81d386fa020d",
       "queued_time": "2026-02-06T01:38:31.0776453Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_csv_to_delta(file_path: str, table_name: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Reads a CSV file from the Lakehouse 'Files' section and saves it as a Delta Table.\n",
    "    \n",
    "    Parameters:\n",
    "        file_path (str): Relative path to the CSV (e.g., '/lakehouse/default/Files/KaggleData/').\n",
    "        table_name (str): The name of the destination Delta table.\n",
    "        mode (str): 'overwrite' to replace the table, default.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        df = (spark.read.format(\"csv\")\n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"multiLine\", \"true\")\n",
    "            .option(\"quote\", \"\\\"\")\n",
    "            .option(\"escape\", \"\\\"\")\n",
    "            .option(\"inferSchema\", \"false\")\n",
    "            .load(file_path)\n",
    "        )\n",
    "\n",
    "        # Get the loaded file count\n",
    "        file_row_count = df.count()\n",
    "\n",
    "        # 2. Save as a Delta Table \n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(mode) \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(table_name)\n",
    "\n",
    "        print(f\"Table '{table_name}' created successfully from {file_path}!\")\n",
    "\n",
    "        delta_df = spark.table(table_name)\n",
    "\n",
    "        # Get the loaded table count\n",
    "        delta_row_count = delta_df.count()\n",
    "        print(f\"Total rows compare : read: {file_row_count}, Delta table: {delta_row_count}\")\n",
    "        \n",
    "        # Assertion Check all data are loaded as per expected\n",
    "        assert_expected_rows_loaded(file_path, table_name, file_row_count, delta_row_count)\n",
    "        # assert file_row_count == delta_row_count, f\"Row count mismatch for {file_path} and {table_name}! Read: {file_row_count}, Delta: {delta_row_count}\"        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table {table_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d250ec34-a9f0-4ea2-b47f-511707b2b3e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:39:14.9531263Z",
       "execution_start_time": "2026-02-06T01:39:14.6499301Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "6b26e201-82ea-4d1c-8c2d-da0a128f0b70",
       "queued_time": "2026-02-06T01:38:31.3528064Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def assert_expected_rows_loaded(scr_name: str, target_name:str, before_rows_count: int, loaded_rows_count: int):\n",
    "    \"\"\"\n",
    "    Validates data integrity by comparing source record counts against loaded records.\n",
    "    \n",
    "    This assertion acts as a 'circuit breaker' in the ETL pipeline. If the counts \n",
    "    do not match exactly, it raises an AssertionError to prevent downstream \n",
    "    processing of incomplete or corrupted data.\n",
    "\n",
    "    Args:\n",
    "        src_name (str): The name of the source system or table (for logging).\n",
    "        target_name (str): The name of the destination Lakehouse/Warehouse table.\n",
    "        before_rows_count (int): The number of records extracted from the source.\n",
    "        loaded_rows_count (int): The number of records successfully written to the target.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the source count and loaded count are not identical.\n",
    "    \"\"\"\n",
    "    assert before_rows_count == loaded_rows_count, f\"Row count mismatch for {scr_name} and {target_name}! Read: {before_rows_count}, Delta: {loaded_rows_count}\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cd0368-f3ff-43e2-b773-c6296c67bbf1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:39:15.2367175Z",
       "execution_start_time": "2026-02-06T01:39:14.9556823Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c34a046d-9dd9-4290-a125-bc4217a190b5",
       "queued_time": "2026-02-06T01:38:31.6055371Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_bronze_delta_tables():\n",
    "    \"\"\"\n",
    "    Orchestrates the loading of raw Olist E-commerce CSV files into Delta tables.\n",
    "    \n",
    "    This function serves as a wrapper to call load_csv_to_delta for each \n",
    "    individual file in the dataset, ensuring a consistent 'Bronze' naming convention.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load Customers dataset\n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_customers_dataset.csv\",\n",
    "        table_name=\"customers_bronze\"\n",
    "    )\n",
    "    \n",
    "    # Load Geolocation dataset\n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_geolocation_dataset.csv\",\n",
    "        table_name=\"geolocation_bronze\"\n",
    "    )\n",
    "\n",
    "    # Load Order_Items dataset      \n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_order_items_dataset.csv\",\n",
    "        table_name=\"order_items_bronze\"\n",
    "    )\n",
    "\n",
    "    # Load Order_Reviews dataset  \n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_order_reviews_dataset.csv\",\n",
    "        table_name=\"order_reviews_bronze\"\n",
    "    )\n",
    "\n",
    "    # Load Order_Payments dataset  \n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_order_payments_dataset.csv\",\n",
    "        table_name=\"order_payments_bronze\"\n",
    "    )\n",
    "\n",
    "    # Load Orders dataset\n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_orders_dataset.csv\",\n",
    "        table_name=\"orders_bronze\"\n",
    "    )\n",
    "\n",
    "    # Load Sellers dataset\n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_sellers_dataset.csv\",\n",
    "        table_name=\"sellers_bronze\"\n",
    "    )\n",
    "\n",
    "    # Load Products dataset\n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/olist_products_dataset.csv\",\n",
    "        table_name=\"products_bronze\"\n",
    "    )\n",
    "\n",
    "    # Load Product_Category_Name_Translation dataset\n",
    "    load_csv_to_delta(\n",
    "        file_path=\"Files/KaggleData/product_category_name_translation.csv\",\n",
    "        table_name=\"product_category_name_translation_bronze\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c20d16f7-12c5-4613-a17c-56e5338b124c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:41:42.0393188Z",
       "execution_start_time": "2026-02-06T01:39:15.2387584Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "205674ac-305d-48a7-a074-8164661e3aaa",
       "queued_time": "2026-02-06T01:38:31.891374Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset 'olistbr/brazilian-ecommerce' to /lakehouse/default/Files/KaggleData/...\n",
      "Dataset URL: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
      "Download and extraction complete!\n",
      "Table 'customers_bronze' created successfully from Files/KaggleData/olist_customers_dataset.csv!\n",
      "Total rows compare : read: 99441, Delta table: 99441\n",
      "Table 'geolocation_bronze' created successfully from Files/KaggleData/olist_geolocation_dataset.csv!\n",
      "Total rows compare : read: 1000163, Delta table: 1000163\n",
      "Table 'order_items_bronze' created successfully from Files/KaggleData/olist_order_items_dataset.csv!\n",
      "Total rows compare : read: 112650, Delta table: 112650\n",
      "Table 'order_reviews_bronze' created successfully from Files/KaggleData/olist_order_reviews_dataset.csv!\n",
      "Total rows compare : read: 99224, Delta table: 99224\n",
      "Table 'order_payments_bronze' created successfully from Files/KaggleData/olist_order_payments_dataset.csv!\n",
      "Total rows compare : read: 103886, Delta table: 103886\n",
      "Table 'orders_bronze' created successfully from Files/KaggleData/olist_orders_dataset.csv!\n",
      "Total rows compare : read: 99441, Delta table: 99441\n",
      "Table 'sellers_bronze' created successfully from Files/KaggleData/olist_sellers_dataset.csv!\n",
      "Total rows compare : read: 3095, Delta table: 3095\n",
      "Table 'products_bronze' created successfully from Files/KaggleData/olist_products_dataset.csv!\n",
      "Total rows compare : read: 32951, Delta table: 32951\n",
      "Table 'product_category_name_translation_bronze' created successfully from Files/KaggleData/product_category_name_translation.csv!\n",
      "Total rows compare : read: 71, Delta table: 71\n",
      "Full Bronze Ingestion Pipeline completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MAIN EXECUTION FLOW\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. Global Configurations\n",
    "# Define the source Kaggle slug and the absolute path in the Fabric OneLake\n",
    "\n",
    "kaggle_dataset_name: str = 'olistbr/brazilian-ecommerce'\n",
    "files_local_path:str = '/lakehouse/default/Files/KaggleData/'\n",
    "\n",
    "# 2. Ingestion Phase\n",
    "extract_kaggle_dataset(kaggle_dataset_name, files_local_path)\n",
    "\n",
    "# 3. Storage Phase\n",
    "load_bronze_delta_tables()\n",
    "\n",
    "print(\"Full Bronze Ingestion Pipeline completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a7c6c2-189c-4fd7-936d-680cea45dc26",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-06T01:41:43.6826695Z",
       "execution_start_time": "2026-02-06T01:41:42.0418731Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "1b77b6c2-1a6c-4333-af72-cee6ae4572fc",
       "queued_time": "2026-02-06T01:38:32.1763772Z",
       "session_id": "e49eef65-93fe-4723-b2b0-c162009589c0",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 16,
       "statement_ids": [
        16
       ]
      },
      "text/plain": [
       "StatementMeta(, e49eef65-93fe-4723-b2b0-c162009589c0, 16, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stop spark session before exit\n",
    "mssparkutils.session.stop()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "791a60f4-f8f8-4ea5-9519-0426798180f4",
    "default_lakehouse_name": "BronzeLakeHouse",
    "default_lakehouse_workspace_id": "7d303a82-1fe7-4f4b-82d6-d23015b8c472",
    "known_lakehouses": [
     {
      "id": "791a60f4-f8f8-4ea5-9519-0426798180f4"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
